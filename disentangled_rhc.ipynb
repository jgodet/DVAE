{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : start\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "print(\"test : start\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, mean_squared_error\n",
    "\n",
    "# my modules :\n",
    "from custom_data_set_class import CustomDataset\n",
    "import custom_vae_module\n",
    "from custom_vae_module import train_modular_vae\n",
    "import custom_vae_module_with_yhat \n",
    "from custom_vae_module_with_yhat import train_modular_vae_with_yhat\n",
    "import my_mlp_module \n",
    "\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using device:', device)\n",
    "print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders \n",
    "batch_size = 128\n",
    "\n",
    "use_rhc_data = False\n",
    "if use_rhc_data :\n",
    "    csv_file_path = \"rhc_preprocessed.csv\" # all columns need to be numeric at that point.\n",
    "    full_dataset = CustomDataset(csv_file_path, do_fillna=True)\n",
    "    train_dataset, test_dataset = train_test_split(full_dataset, test_size=0.2, random_state=42)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    input_dim = 122\n",
    "    hidden_dim = 65\n",
    "    latent_dim = 15\n",
    "    num_labels = 2\n",
    "else : \n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "    test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "    input_dim=784\n",
    "    hidden_dim=300\n",
    "    latent_dim=10\n",
    "    num_labels=10\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "use_model_with_yhat = False\n",
    "if use_model_with_yhat : \n",
    "    model = custom_vae_module_with_yhat.ModularVAEWithYHat(input_dim, hidden_dim, latent_dim, num_labels, debug_mode = False).to(device)\n",
    "    train_my_model = train_modular_vae_with_yhat    \n",
    "else : \n",
    "    model = custom_vae_module.ModularVAE(input_dim, hidden_dim, latent_dim, num_labels, debug_mode = False).to(device)\n",
    "    train_my_model = train_modular_vae\n",
    "\n",
    "\n",
    "\n",
    "p_input_dim = input_dim\n",
    "p_hidden_dim = 120\n",
    "p_num_labels = num_labels\n",
    "\n",
    "my_mlp_from_x = my_mlp_module.MyMLP(input_dim = p_input_dim,  num_labels = p_num_labels ).to(device)\n",
    "my_mlp_from_z = my_mlp_module.MyMLPFromLatentSpace(model, input_dim = p_input_dim,  num_labels = p_num_labels , z_dim=latent_dim).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "mlp_optimizer_1 = optim.Adam(my_mlp_from_x.parameters(), lr=1e-3)\n",
    "mlp_optimizer_z = optim.Adam(my_mlp_from_z.parameters(), lr=1e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 44.3029657796224\n",
      "====> Epoch: 2 Average loss: 32.75149049479167\n",
      "====> Epoch: 3 Average loss: 30.917134025065103\n",
      "====> Epoch: 4 Average loss: 29.94239003092448\n",
      "====> Epoch: 5 Average loss: 29.25592264811198\n",
      "====> Epoch: 6 Average loss: 28.784832853190103\n",
      "====> Epoch: 7 Average loss: 28.380687520345052\n",
      "====> Epoch: 8 Average loss: 28.110269685872396\n",
      "====> Epoch: 9 Average loss: 27.879476883951824\n",
      "====> Epoch: 10 Average loss: 27.692205249023438\n",
      "====> Epoch: 11 Average loss: 27.504765856933595\n",
      "====> Epoch: 12 Average loss: 27.363344795735678\n",
      "====> Epoch: 13 Average loss: 27.23871006673177\n",
      "====> Epoch: 14 Average loss: 27.121190649414064\n",
      "====> Epoch: 15 Average loss: 27.003873352050782\n",
      "====> Epoch: 16 Average loss: 26.94264547932943\n",
      "====> Epoch: 17 Average loss: 26.870927604166667\n",
      "====> Epoch: 18 Average loss: 26.776848327636717\n",
      "====> Epoch: 19 Average loss: 26.723350174967447\n",
      "====> Epoch: 20 Average loss: 26.664193819173178\n",
      "====> Epoch: 21 Average loss: 26.591112475585938\n",
      "====> Epoch: 22 Average loss: 26.559298014322916\n",
      "====> Epoch: 23 Average loss: 26.47945495605469\n",
      "====> Epoch: 24 Average loss: 26.429997534179687\n",
      "====> Epoch: 25 Average loss: 26.38717107747396\n",
      "====> Epoch: 26 Average loss: 26.33710876871745\n",
      "====> Epoch: 27 Average loss: 26.28165175374349\n",
      "====> Epoch: 28 Average loss: 26.274408211263022\n",
      "====> Epoch: 29 Average loss: 26.24226465657552\n",
      "====> Epoch: 30 Average loss: 26.18706055908203\n",
      "====> Epoch: 31 Average loss: 26.124152600097656\n",
      "====> Epoch: 32 Average loss: 26.117028572591146\n",
      "====> Epoch: 33 Average loss: 26.081450443522137\n",
      "====> Epoch: 34 Average loss: 26.062079243977866\n",
      "====> Epoch: 35 Average loss: 26.035715397135416\n",
      "====> Epoch: 36 Average loss: 25.977256168619792\n",
      "====> Epoch: 37 Average loss: 25.95979443359375\n",
      "====> Epoch: 38 Average loss: 25.929285872395834\n",
      "====> Epoch: 39 Average loss: 25.913589876302083\n",
      "====> Epoch: 40 Average loss: 25.88610529785156\n",
      "====> Epoch: 41 Average loss: 25.864564432779947\n",
      "====> Epoch: 42 Average loss: 25.80202174886068\n",
      "====> Epoch: 43 Average loss: 25.796256856282554\n",
      "====> Epoch: 44 Average loss: 25.76625265299479\n",
      "====> Epoch: 45 Average loss: 25.75737373860677\n",
      "====> Epoch: 46 Average loss: 25.73122996826172\n",
      "====> Epoch: 47 Average loss: 25.72227854410807\n",
      "====> Epoch: 48 Average loss: 25.72163329264323\n",
      "====> Epoch: 49 Average loss: 25.6811735148112\n",
      "====> Epoch: 50 Average loss: 25.675119966634114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run training\n",
    "epochs_main = 50\n",
    "beta = 1 \n",
    "for epoch in range(1, epochs_main + 1):\n",
    "    train_my_model(model, train_loader, optimizer, epoch, beta, device)\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), 'vae_model_with_custom_loss.pth')\n",
    "torch.save(model, 'beta_vae_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.0026038405789683265\n",
      "====> Epoch: 2 Average loss: 0.001081596751511097\n",
      "====> Epoch: 3 Average loss: 0.0007129103180486709\n",
      "====> Epoch: 4 Average loss: 0.0005242658252362162\n",
      "====> Epoch: 5 Average loss: 0.0003916715833048026\n",
      "====> Epoch: 6 Average loss: 0.0003060256991923476\n",
      "====> Epoch: 7 Average loss: 0.00023236453636394194\n",
      "====> Epoch: 8 Average loss: 0.00017787049792241305\n",
      "====> Epoch: 9 Average loss: 0.00013686733721600224\n",
      "====> Epoch: 10 Average loss: 0.00010656199979130179\n",
      "====> Epoch: 11 Average loss: 8.710714701446705e-05\n",
      "====> Epoch: 12 Average loss: 7.171888598725976e-05\n",
      "====> Epoch: 13 Average loss: 5.526086665049661e-05\n",
      "====> Epoch: 14 Average loss: 4.74082966131391e-05\n",
      "====> Epoch: 15 Average loss: 3.381610758709333e-05\n",
      "====> Epoch: 16 Average loss: 4.801112780890738e-05\n",
      "====> Epoch: 17 Average loss: 4.541870199488282e-05\n",
      "====> Epoch: 18 Average loss: 1.8636670926207443e-05\n",
      "====> Epoch: 19 Average loss: 6.948488977529147e-06\n",
      "====> Epoch: 20 Average loss: 6.630105695755144e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run training\n",
    "epochs_mlp_1 = 20\n",
    "beta = 1 \n",
    "for epoch in range(1, epochs_mlp_1 + 1):\n",
    "    my_mlp_from_x.train_myself(train_loader, mlp_optimizer_1, epoch,  device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.01799553252061208\n",
      "====> Epoch: 2 Average loss: 0.01799535853068034\n",
      "====> Epoch: 3 Average loss: 0.017995671713352205\n",
      "====> Epoch: 4 Average loss: 0.01799514531691869\n",
      "====> Epoch: 5 Average loss: 0.017995608520507814\n",
      "====> Epoch: 6 Average loss: 0.017994524069627125\n",
      "====> Epoch: 7 Average loss: 0.017994767618179322\n",
      "====> Epoch: 8 Average loss: 0.01799535903930664\n",
      "====> Epoch: 9 Average loss: 0.017995014123121898\n",
      "====> Epoch: 10 Average loss: 0.017994966940085094\n",
      "====> Epoch: 11 Average loss: 0.017995028416315714\n",
      "====> Epoch: 12 Average loss: 0.01799441419442495\n",
      "====> Epoch: 13 Average loss: 0.017994992307821908\n",
      "====> Epoch: 14 Average loss: 0.017995308645566303\n",
      "====> Epoch: 15 Average loss: 0.017996103517214458\n",
      "====> Epoch: 16 Average loss: 0.01799531391064326\n",
      "====> Epoch: 17 Average loss: 0.017994798747698465\n",
      "====> Epoch: 18 Average loss: 0.017995005389054616\n",
      "====> Epoch: 19 Average loss: 0.01799539540608724\n",
      "====> Epoch: 20 Average loss: 0.017995916561285654\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Run training\n",
    "epochs_mlp_z = 20\n",
    "beta = 1 \n",
    "for epoch in range(1, epochs_mlp_z + 1):\n",
    "    my_mlp_from_z.train_myself(train_loader, optimizer =  mlp_optimizer_1, epoch = epoch, device = device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9775\n",
      "Precision: 0.9780\n",
      "Recall: 0.9775\n",
      "F1 Score: 0.9775\n",
      "Confusion Matrix:\n",
      "[[ 970    0    0    2    0    0    4    0    2    2]\n",
      " [   0 1124    3    1    0    1    2    1    3    0]\n",
      " [   3    2 1012    3    2    0    2    3    4    1]\n",
      " [   0    0    2  995    0    2    0    1    0   10]\n",
      " [   1    0    2    0  964    0    1    0    0   14]\n",
      " [   2    0    0    5    1  874    4    0    5    1]\n",
      " [   4    2    2    1    4    3  941    0    1    0]\n",
      " [   0    6   16    5    4    0    0  945    3   49]\n",
      " [   1    0    3    3    3    4    0    0  955    5]\n",
      " [   0    2    0    2    6    3    0    0    1  995]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.97      0.98      0.98      1032\n",
      "           3       0.98      0.99      0.98      1010\n",
      "           4       0.98      0.98      0.98       982\n",
      "           5       0.99      0.98      0.98       892\n",
      "           6       0.99      0.98      0.98       958\n",
      "           7       0.99      0.92      0.96      1028\n",
      "           8       0.98      0.98      0.98       974\n",
      "           9       0.92      0.99      0.95      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "returning : precision, recall, f1, conf_matrix, class_report\n"
     ]
    }
   ],
   "source": [
    "mlp_evaluation = my_mlp_from_x.evaluation(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1027\n",
      "Precision: 0.0639\n",
      "Recall: 0.1027\n",
      "F1 Score: 0.0546\n",
      "Confusion Matrix:\n",
      "[[  0 385  35   0   1   0  50 479   0  30]\n",
      " [  0 372  39   0  17   0  80 598   0  29]\n",
      " [  0 525  42   0  13   0  64 373   0  15]\n",
      " [  0 393  40   0  17   0  67 479   0  14]\n",
      " [  0 375  17   0  11   0 141 416   0  22]\n",
      " [  0 337   7   0  23   0  73 426   0  26]\n",
      " [  1 337  21   0   6   0  68 510   0  15]\n",
      " [  0 321  12   0  17   0 117 515   0  46]\n",
      " [  0 405  19   0  28   0  74 425   0  23]\n",
      " [  0 334   4   0  17   0  75 560   0  19]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       980\n",
      "           1       0.10      0.33      0.15      1135\n",
      "           2       0.18      0.04      0.07      1032\n",
      "           3       0.00      0.00      0.00      1010\n",
      "           4       0.07      0.01      0.02       982\n",
      "           5       0.00      0.00      0.00       892\n",
      "           6       0.08      0.07      0.08       958\n",
      "           7       0.11      0.50      0.18      1028\n",
      "           8       0.00      0.00      0.00       974\n",
      "           9       0.08      0.02      0.03      1009\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.06      0.10      0.05     10000\n",
      "weighted avg       0.06      0.10      0.05     10000\n",
      "\n",
      "returning : precision, recall, f1, conf_matrix, class_report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulm/Bureau/codes/rhc_dataset/my_venv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/paulm/Bureau/codes/rhc_dataset/my_venv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/paulm/Bureau/codes/rhc_dataset/my_venv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/paulm/Bureau/codes/rhc_dataset/my_venv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "mlp_z_evaluation = my_mlp_from_z.evaluation(test_loader, device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_x = []\n",
    "all_y = []\n",
    "all_x_hat = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        x_hat, _ , _, _ = model(data, label)\n",
    "        all_y.extend(label.cpu().numpy())\n",
    "        all_x.extend(data.cpu().numpy())\n",
    "        all_x_hat.extend(x_hat.cpu().numpy())\n",
    "\n",
    "all_x = np.array(all_x)\n",
    "all_y = np.array(all_y)\n",
    "all_x_hat = np.array(all_x_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x_hat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996\n",
      "Precision: 0.9961\n",
      "Recall: 0.9960\n",
      "F1 Score: 0.9960\n",
      "Confusion Matrix:\n",
      "[[ 980    0    0    0    0    0    0    0    0    0]\n",
      " [   0 1135    0    0    0    0    0    0    0    0]\n",
      " [   0    0 1030    1    0    0    0    1    0    0]\n",
      " [   0    0    0 1010    0    0    0    0    0    0]\n",
      " [   0    0    0    0  981    0    0    0    0    1]\n",
      " [   0    0    0    1    0  891    0    0    0    0]\n",
      " [   0    0    0    0    0    0  958    0    0    0]\n",
      " [   0    0    0    0    0    0    0  992    0   36]\n",
      " [   0    0    0    0    0    0    0    0  974    0]\n",
      " [   0    0    0    0    0    0    0    0    0 1009]]\n"
     ]
    }
   ],
   "source": [
    "my_mlp_from_x.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test = torch.from_numpy(all_x_hat).to(device)\n",
    "    # test.detach().numpy()\n",
    "\n",
    "    all_y_hat = my_mlp_from_x.forward(test)\n",
    "\n",
    "\n",
    "    all_y_hat = torch.argmax(all_y_hat, dim=1).cpu().detach().numpy()\n",
    "    accuracy = accuracy_score(all_y, all_y_hat)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "    precision = precision_score(all_y, all_y_hat, average='weighted')\n",
    "    recall = recall_score(all_y, all_y_hat, average='weighted')\n",
    "    f1 = f1_score(all_y, all_y_hat, average='weighted')\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(all_y, all_y_hat)\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABFwAAAH4CAYAAAB3zPmWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxkElEQVR4nO3de5jVdZ0H8M+ZGRguI8PIgNwvguK1VAxWZcBLLesl0xQXcxG85DUvrVn2VN7q0dDa2K0k3TUraXs2KQ03zUualyQzUzBNRQuNAgvkDgPMzG//8JlZhgHE+X5tVF6v5+l56nfO5zvvc2bo9533/M45paIoigAAAAAgm7KODgAAAADwXqNwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwYYdw5ZVXRqlUatfsd77znSiVSrFgwYK8oTaxYMGCKJVK8Z3vfOdt+xpvt+bH8JWvfKWjowAAJLM/61hvZf/efN8lS5a8zaneXPPvDr/5zW+yrZnyuwwdS+HCO9qzzz4b//Iv/xIDBgyIysrK6N+/f5xyyinx7LPPdnS0DvGLX/wiSqVSlEqlePLJJ9vcPnXq1KiqquqAZO8cM2bMiIkTJ8bgwYOjVCrF1KlTOzoSADuQ5l+2mv9TUVERAwYMiKlTp8af//znjo6X3Q033NDhhURHZ7A/+/u55ppr4o477si+7nvle3T77bfHhAkTon///lFZWRkDBw6ME088MX73u991dLQdlsKFd6wf//jHccABB8TPf/7zOO200+KGG26IM844Ix588ME44IAD4vbbb9/utT7/+c/HunXr2pVj8uTJsW7duhgyZEi75t8uV155ZUdHeEeaNm1aPPDAA7H33ntHRUVFR8cBYAd19dVXx6233hrf+ta34sgjj4yZM2fG+PHjo76+vqOjZdXRZcc7JUMz+7N8trR/f7sKl/eKZ555JmpqauKiiy6KG264Ic4999x46qmnYvTo0TF37tyOjrdD8tsI70gvv/xyTJ48OXbdddd4+OGHo3fv3i23XXTRRVFXVxeTJ0+OefPmxa677rrVddasWRPdu3ePioqKdv/yXV5eHuXl5e2afbvst99+8b//+7/x29/+Ng444ICOjvN31fw93ZqHHnqo5eqW98JfKgB4dzryyCPjwAMPjIiIM888M2pra2PatGkxe/bsOOmkkzo4Xcd4s3P4u539Wd7vbcr+fUd1+eWXtzl25plnxsCBA2PGjBnxrW99qwNS7dhc4cI70vXXXx9r166Nm266qVXZEhFRW1sbN954Y6xZsyauu+66luPNr2187rnn4mMf+1jU1NTE2LFjW922qXXr1sWFF14YtbW1sdNOO8Wxxx4bf/7zn6NUKrX668SW3sNl6NChccwxx8Sjjz4ao0ePji5dusSuu+4a3/ve91p9jddffz0+9alPxb777htVVVXRo0ePOPLII5Mb5gsuuCBqamq2668omz+eTR/Dpi+3aX6cjz76aFx44YXRu3fv6NmzZ5x99tmxYcOGWL58eZx66qlRU1MTNTU18elPfzqKotji1/za174WQ4YMia5du8b48eO3eBnj888/HyeeeGLsvPPO0aVLlzjwwANj9uzZre7TnOmhhx6K8847L/r06RMDBw7c5uMdMmSI17gC8I5TV1cXEW/8UWlT23M+jIhYvnx5fPKTn4yhQ4e2vFTg1FNPbfWeFX/961/jjDPOiF122SW6dOkS73//++O73/1uq3U2fU+Pm266KYYPHx6VlZXxgQ98IJ544olW9128eHGcdtppMXDgwKisrIx+/frFRz7ykZY90dChQ+PZZ5+Nhx56qOUlNYceemhEbPscPnXq1Bg6dGibx7i196mYOXNmjB49Orp16xY1NTUxbty4uPfee980Q/PzdvHFF8egQYOisrIyRowYEdOmTYumpqY2z+/UqVOjuro6evbsGVOmTInly5e3ybIt9mdtFUURtbW18a//+q8tx5qamqJnz55RXl7e6jmeNm1aVFRUxOrVqyOi7c9DqVSKNWvWxHe/+92W7/XmLx1v/j727Nkzqqur47TTTou1a9duMdtb9corr8R5550XI0eOjK5du0avXr1i4sSJW32fx7Vr18bZZ58dvXr1ih49esSpp54ay5Yta3O/u+++O+rq6qJ79+6x0047xdFHH5317RP69OkT3bp1e8s/z+ShMuQd6c4774yhQ4e2bE42N27cuBg6dGj89Kc/bXPbxIkTY7fddotrrrlmqyeciDdO9j/84Q9j8uTJ8Q//8A/x0EMPxdFHH73dGV966aU48cQT44wzzogpU6bEt7/97Zg6dWqMGjUq9t5774iI+MMf/hB33HFHTJw4MYYNGxavvfZa3HjjjTF+/Ph47rnnon///tv99TbVo0eP+OQnPxmXX3559r+iXHDBBdG3b9+46qqr4le/+lXcdNNN0bNnz3jsscdi8ODBcc0118Rdd90V119/feyzzz5x6qmntpr/3ve+F6tWrYrzzz8/6uvr49///d/j8MMPj2eeeSZ22WWXiHjjvXkOOeSQGDBgQFx22WXRvXv3+OEPfxjHHXdc/OhHP4rjjz++1ZrnnXde9O7dOy6//PJYs2ZNtscKAH8vzb+U1dTUtBzb3vPh6tWro66uLn7/+9/H6aefHgcccEAsWbIkZs+eHQsXLoza2tpYt25dHHroofHSSy/FJz7xiRg2bFjcdtttMXXq1Fi+fHlcdNFFrfL893//d6xatSrOPvvsKJVKcd1118VHP/rR+MMf/hCdOnWKiIgTTjghnn322bjgggti6NCh8de//jXuu+++ePXVV2Po0KExffr0uOCCC6Kqqio+97nPRUS0nOubpZ7Dr7rqqrjyyivj4IMPjquvvjo6d+4cjz/+eDzwwAPxj//4j9vMsHbt2hg/fnz8+c9/jrPPPjsGDx4cjz32WHz2s5+NRYsWxfTp0yPijVLgIx/5SDz66KNxzjnnxJ577hm33357TJky5S1ltT9rq1QqxSGHHBIPP/xwy7F58+bFihUroqysLH75y1+27L8feeSR2H///bd6hfKtt94aZ555ZowePTrOOuusiIgYPnx4q/ucdNJJMWzYsLj22mvjt7/9bfzXf/1X9OnTJ6ZNm/YWnu0te+KJJ+Kxxx6LSZMmxcCBA2PBggUxY8aMOPTQQ+O5556Lbt26tbr/Jz7xiejZs2dceeWV8cILL8SMGTPilVdeaXnPn+bHNGXKlJgwYUJMmzYt1q5dGzNmzIixY8fGU089tcVicnssX748Nm7cGIsXL47p06fHypUr44gjjkh9CmiPAt5hli9fXkRE8ZGPfGSb9zv22GOLiChWrlxZFEVRXHHFFUVEFCeffHKb+zbf1uzJJ58sIqK4+OKLW91v6tSpRUQUV1xxRcuxW265pYiI4o9//GPLsSFDhhQRUTz88MMtx/76178WlZWVxSWXXNJyrL6+vmhsbGz1Nf74xz8WlZWVxdVXX93qWEQUt9xyyzYf84MPPlhERHHbbbcVy5cvL2pqaopjjz225fYpU6YU3bt3bzWz+ePZ9DFMmTKlzeOcMGFC0dTU1HL8oIMOKkqlUnHOOee0HGtoaCgGDhxYjB8/vs1j6Nq1a7Fw4cKW448//ngREcUnP/nJlmNHHHFEse+++xb19fUtx5qamoqDDz642G233dpkGjt2bNHQ0LDN52ZLunfv3uoxAsDbrfncdf/99xd/+9vfij/96U/FrFmzit69exeVlZXFn/70p5b7bu/58PLLLy8iovjxj3/c5us1n7OnT59eREQxc+bMlts2bNhQHHTQQUVVVVXLfqn5fN2rV6/i9ddfb7nvT37ykyIiijvvvLMoiqJYtmxZERHF9ddfv83Hu/fee7faD2z+PGzpHD5lypRiyJAhbWY236/Nnz+/KCsrK44//vg2+6lN9ypby/DFL36x6N69e/Hiiy+2On7ZZZcV5eXlxauvvloURVHccccdRUQU1113Xct9Ghoairq6OvuzDPuz66+/vigvL2/5GfyP//iPYsiQIcXo0aOLz3zmM0VRFEVjY2PRs2fPVnk2/3koiq3v7Zrve/rpp7c6fvzxxxe9evV604xb+h5tbu3atW2OzZkzp4iI4nvf+17LsebnZ9SoUcWGDRtajl933XVFRBQ/+clPiqIoilWrVhU9e/YsPv7xj7dac/HixUV1dXWr41t6LrZl5MiRRUQUEVFUVVUVn//859v8G+Lvw0uKeMdZtWpVRETstNNO27xf8+0rV65sdfycc85506/xs5/9LCLeaOY3dcEFF2x3zr322qvVFTi9e/eOkSNHxh/+8IeWY5WVlVFW9sY/s8bGxli6dGlUVVXFyJEj47e//e12f60tqa6ujosvvjhmz54dTz31VNJamzrjjDNaXb45ZsyYKIoizjjjjJZj5eXlceCBB7Z6rM2OO+64GDBgQMv/Hj16dIwZMybuuuuuiHjjZVYPPPBAnHTSSbFq1apYsmRJLFmyJJYuXRoTJkyI+fPnt/kUh49//OPvuPfRAYBt+eAHPxi9e/eOQYMGxYknnhjdu3eP2bNnt7z04q2cD3/0ox/F+9///jZXGEREyzn7rrvuir59+8bJJ5/cclunTp3iwgsvjNWrV8dDDz3Uau6f//mfW11t07ynaT63d+3aNTp37hy/+MUvtvgyiO2Vcg6/4447oqmpKS6//PKW/VSz7Xn58G233RZ1dXVRU1PT8vwuWbIkPvjBD0ZjY2PLVRd33XVXVFRUxLnnntsyW15e/pb2hc3sz9qqq6uLxsbGeOyxxyLijStZ6urqoq6uLh555JGIiPjd734Xy5cv3+rV7dtr898D6urqYunSpW1+X2iPrl27tvz3jRs3xtKlS2PEiBHRs2fPLe7rzzrrrJarxSIizj333KioqGh5zu+7775Yvnx5nHzyya1+PsvLy2PMmDHx4IMPtjvrLbfcEj/72c/ihhtuiD333DPWrVsXjY2N7V6P9lO48I7TXKQ0Fy9bs7ViZtiwYW/6NV555ZUoKytrc98RI0Zsd87Bgwe3OVZTU9NqU9LU1BRf+9rXYrfddovKysqora2N3r17t1xKmeqiiy5quVQxl80fV3V1dUREDBo0qM3xLW3AdttttzbHdt9995ZLqV966aUoiiK+8IUvRO/evVv954orroiIN16Dvqnt+Z4CwDvJN7/5zbjvvvti1qxZcdRRR8WSJUuisrKy5fa3cj58+eWXY5999tnm13vllVdit912a1NM7Lnnni23b2rz831z+dJ8bq+srIxp06bF3XffHbvsskuMGzcurrvuuli8ePFbeh5SzuEvv/xylJWVxV577dWu+fnz58fPfvazNs/vBz/4wYj4/+f3lVdeiX79+rV5KcvIkSPb9XXtz1o74IADolu3bi3lSnPhMm7cuPjNb34T9fX1Lbc1v/9ie73Zz3WKdevWxeWXX97yfkDN+/rly5dvcV+/+XNeVVUV/fr1a3nO58+fHxERhx9+eJvn/N57723zfL8VBx10UEyYMCHOPffcuOeee2LmzJnx2c9+tt3r0X7ew4V3nOrq6ujXr1/Mmzdvm/ebN29eDBgwIHr06NHq+Kbt89tpa41+scn7xlxzzTXxhS98IU4//fT44he/GDvvvHOUlZXFxRdf3ObN2tqj+a8oV1555Vv+K8rWWu6tPa4tHS+28R45W9P8uD/1qU/FhAkTtnifzYuvv9f3FAByGT16dMunFB133HExduzY+NjHPhYvvPBCVFVVtet8mNP27GMuvvji+PCHPxx33HFH3HPPPfGFL3whrr322njggQdi//33366vs6Vz+NauTsn9F/impqb40Ic+FJ/+9Ke3ePvuu++e9es1sz9rrVOnTjFmzJh4+OGH46WXXorFixdHXV1d7LLLLrFx48Z4/PHH45FHHok99tijzYdlvFXb83PdXhdccEHccsstcfHFF8dBBx0U1dXVUSqVYtKkSe3a1zfP3HrrrdG3b982t+f6hKaampo4/PDD4/vf/3585StfybIm20/hwjvSMcccE//5n/8Zjz766Bab7kceeSQWLFgQZ599drvWHzJkSDQ1NcUf//jHVu3zSy+91O7MWzJr1qw47LDD4uabb251fPny5VFbW5vla1x88cUxffr0uOqqq6Jnz55tbq+pqWnzruQbNmyIRYsWZfn6m2tu6zf14osvtrzpV/PHeHfq1KnlL0wA8F5WXl4e1157bRx22GHxjW98Iy677LK3dD4cPnz4Fj9RZlNDhgyJefPmRVNTU6urXJ5//vmW29tj+PDhcckll8Qll1wS8+fPj/322y+++tWvxsyZMyNi+17as7kt7U0i2l6FM3z48Ghqaornnnsu9ttvv62ut7UMw4cPj9WrV7/p8ztkyJD4+c9/HqtXr251lcsLL7ywzbltsT9rra6uLqZNmxb3339/1NbWxh577BGlUin23nvveOSRR+KRRx6JY4455k3X6chPopw1a1ZMmTIlvvrVr7Ycq6+v3+qn/8yfPz8OO+ywlv+9evXqWLRoURx11FER8f9v+NunT5+3fU+8bt26LFfX89Z5SRHvSJdeeml07do1zj777Fi6dGmr215//fU455xzolu3bnHppZe2a/3m5v6GG25odfzrX/96+wJvRXl5eZtG/bbbbmvzGtgUzX9F+clPfhJPP/10m9uHDx/e6p3hIyJuuummt+11nHfccUerx/frX/86Hn/88TjyyCMj4o2TyqGHHho33njjFjcVf/vb396WXADQkQ499NAYPXp0TJ8+Perr69/S+fCEE06IuXPnxu23397mfs37jKOOOioWL14c//M//9NyW0NDQ3z961+PqqqqGD9+/FvKu3bt2qivr291bPjw4bHTTjvF+vXrW4517979LX/c7PDhw2PFihWtrmZetGhRm8d33HHHRVlZWVx99dVtriDYdH+1tQwnnXRSzJkzJ+655542ty1fvjwaGhoi4o3nrqGhIWbMmNFye2NjY9K+0P6stbq6uli/fn1Mnz49xo4d21Kc1NXVxa233hp/+ctftuv9W9rz85bLlvb1X//617f6Pbvpppti48aNLf97xowZ0dDQ0PKcT5gwIXr06BHXXHNNq/s1a89zvqWXIS1YsCB+/vOft1xxx9+XK1x4R9ptt93iu9/9bpxyyimx7777xhlnnBHDhg2LBQsWxM033xxLliyJH/zgB20+Cm57jRo1Kk444YSYPn16LF26tOVjoV988cWIyNeeH3PMMXH11VfHaaedFgcffHA888wz8f3vf7/lrwi5XHTRRfG1r30t5s6dG927d29125lnnhnnnHNOnHDCCfGhD30o5s6dG/fcc0+2K2w2N2LEiBg7dmyce+65LSfWXr16tbqc95vf/GaMHTs29t133/j4xz8eu+66a7z22msxZ86cWLhwYcydO7fdX//OO+9smd+4cWPMmzcvvvSlL0VExLHHHhvve9/70h4gALTTpZdeGhMnTozvfOc7cc4552z3+fDSSy+NWbNmxcSJE+P000+PUaNGxeuvvx6zZ8+Ob33rW/H+978/zjrrrLjxxhtj6tSp8eSTT8bQoUNj1qxZ8ctf/jKmT5/+ph9GsLkXX3wxjjjiiDjppJNir732ioqKirj99tvjtddei0mTJrXcb9SoUTFjxoz40pe+FCNGjIg+ffrE4Ycfvs21J02aFJ/5zGfi+OOPjwsvvLDlo3B33333Vm8+OmLEiPjc5z4XX/ziF6Ouri4++tGPRmVlZTzxxBPRv3//uPbaa7eZ4dJLL43Zs2fHMcccE1OnTo1Ro0bFmjVr4plnnolZs2bFggULora2Nj784Q/HIYccEpdddlksWLAg9tprr/jxj3+cfEWA/dn/O+igg6KioiJeeOGFlo90jogYN25cS9G1PYXLqFGj4v77749/+7d/i/79+8ewYcNizJgx7c61qY0bN7bsGTe18847x3nnnRfHHHNM3HrrrVFdXR177bVXzJkzJ+6///7o1avXFtfbsGFDy7+hF154IW644YYYO3ZsHHvssRHxxseIz5gxIyZPnhwHHHBATJo0KXr37h2vvvpq/PSnP41DDjkkvvGNb7ylx7DvvvvGEUccEfvtt1/U1NTE/Pnz4+abb46NGzfGl7/85bf+pJCuIz4aCbbXvHnzipNPPrno169f0alTp6Jv377FySefXDzzzDNt7tv8cWl/+9vftnrbptasWVOcf/75xc4771xUVVUVxx13XPHCCy8UEVF8+ctfbrnf1j4W+uijj27zdcaPH9/qo/jq6+uLSy65pOjXr1/RtWvX4pBDDinmzJnT5n7t+VjorT3GzT/SrrGxsfjMZz5T1NbWFt26dSsmTJhQvPTSS1v92MEnnnhii+tu/rxu/vF5zY/h+uuvL7761a8WgwYNKiorK4u6urpi7ty5bfK+/PLLxamnnlr07du36NSpUzFgwIDimGOOKWbNmvWmmbZlypQpLR+Dt/l/3uz5BYBU2zp3NTY2FsOHDy+GDx/e8nG623M+LIqiWLp0afGJT3yiGDBgQNG5c+di4MCBxZQpU4olS5a03Oe1114rTjvttKK2trbo3Llzse+++7Y59216vt5cbPJRxUuWLCnOP//8Yo899ii6d+9eVFdXF2PGjCl++MMftppZvHhxcfTRRxc77bRTEREt+5s3O4ffe++9xT777FN07ty5GDlyZDFz5sytfvTtt7/97WL//fcvKisri5qammL8+PHFfffd96YZiuKNj9797Gc/W4wYMaLo3LlzUVtbWxx88MHFV77ylVYf2bt06dJi8uTJRY8ePYrq6upi8uTJxVNPPWV/lml/VhRF8YEPfKCIiOLxxx9vObZw4cIiIopBgwa1uf+Wfh6ef/75Yty4cUXXrl2LiGh5rrb2fGxpH78l29o/Dh8+vCiKNz4qvfnfV1VVVTFhwoTi+eef3+r37KGHHirOOuusoqampqiqqipOOeWUYunSpW2+9oMPPlhMmDChqK6uLrp06VIMHz68mDp1avGb3/xmm8/FllxxxRXFgQceWNTU1BQVFRVF//79i0mTJhXz5s1701neHqWiyPAOQvAe8fTTT8f+++8fM2fOjFNOOaWj4wAAAPAu5T1c2GGtW7euzbHp06dHWVlZjBs3rgMSAQAA8F7hPVzYYV133XXx5JNPxmGHHRYVFRVx9913x9133x1nnXVWDBo0qKPjAQAA8C7mJUXssO6777646qqr4rnnnovVq1fH4MGDY/LkyfG5z30u2+feAwAAsGNSuAAAAABk5j1cAAAAADJTuAAAAABkpnABAAAAyGy73xm0VCq9nTkA4D3H26TxbmGfBwDbb3v3eK5wAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZFbR0QGANKVSqaMjRER6jhyPo7GxMXkNAIB3itT9UVEUmZIA7eEKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYVHR0AOlJZWVrn2L179+QMe+65Z9L8wQcfnJxh2LBhyWvce++9SfO//vWvkzOsWrUqaX7Dhg3JGZqampLXAID2KpVKyWtUVKT9irDzzjsnZ9hrr72S5seNG5ecYcCAAUnzvXv3Ts6wbNmypPn58+cnZ7j77ruT5p9//vnkDOvXr0+aL4oiOQO0hytcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMKjo6AHSkUqmUNN+jR4/kDHvvvXfS/Mknn5ycobq6OnmNTp06Jc0vWrQoOcP8+fOT5jds2JCcAQA6UllZ+t9Te/funTT/0Y9+NDnDxIkTk+ZHjBiRnCF1n1dZWZmcIVV9fX3yGh/+8IeT5r/5zW8mZ7jjjjuS5tesWZOcAdrDFS4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMKjo6ALRXqVRKXqOysjJpfsCAAckZxo4dmzRfW1ubnKFHjx7Ja+yxxx5J8/369UvOsHDhwqT51atXJ2cAgI5UVpb+99Sdd945aX7QoEHJGQYPHpw0n/oYIiIqKtJ+VcrxvUjVuXPn5DX22muvpPnzzz8/OcOKFSuS5u+6667kDE1NTclrsOPp+P8XAAAAAHiPUbgAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzCo6OgC0V6lUSl6jvLw8ab6mpiY5Q2NjY9J8fX19cobOnTsnr7Fs2bKk+YaGhuQMa9euTZoviiI5AwB0pKampuQ1Vq5cmTT/pz/9KTnDwoULk+ZzPA+pe5MVK1YkZ1i/fn3S/C677JKcoVevXknzu+++e3KG448/Pmn+kUceSc6Q4/vJjscVLgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwqOjoAtFdZWXpf2KlTp6T5Pn36JGdYtWpV0nxjY2NyhhyqqqqS5ouiSM5QX1+fvAYAvJs1NTUlr7Fy5cqk+WeffTY5w/333580X1GR/mvOggULkuZfeuml5Aw9evRImj/++OOTM/zTP/1T0vxOO+2UnGHgwIFJ89XV1ckZVqxYkbwGOx5XuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmVV0dAB2XOXl5UnznTt3Ts7Qo0ePpPmmpqbkDIsXL06ab2xsTM7QrVu35DVqamqS5l9//fXkDDm+HwCwo1u7dm3S/PPPP5+cYdGiRUnzOfaJa9asSZpfsWJFcoba2tqk+eXLlydnaGho6ND5HIqiSF6jVCp1eAbefVzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyKyiowOw4yqVSh06HxHR1NSUNL927drkDCtXrkyar6hI/2fctWvX5DU6deqUNN/Q0JCcIfX7CQDvdkVRJK+Rek5O3dtERDQ2NibN59jbbNiwIWm+S5cuyRn23HPPpPlRo0YlZ0h9HBs3bkzO8Je//CVpfvXq1ckZcvzbYsfjChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAILOKjg7AjquxsTFpfu3atckZmpqakubnz5+fnGHgwIFJ8/369UvO0Llz5w5foyiK5AwAQLrUc3JDQ0OHZ+jUqVNyhpqamqT5973vfckZTjzxxKT5wYMHJ2colUpJ80uXLk3O8Mtf/jJpfuXKlckZoD1c4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMisoqMDsOMqiqKjI8TGjRuT5pctW5acoW/fvknzlZWVyRlKpVLyGmvWrEmaX7RoUXIGACBd6h4txx4vdY1BgwYlZxg5cmTS/JFHHpmcYfjw4clrpErd786bNy85w5133pk039jYmJwB2sMVLgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZhUdHQDezRoaGpLX2GmnnZLmu3btmpyhKIrkNR5//PGk+RUrViRnAAA6XlNTU/IaGzduTJpft25dcoYePXokzffr16/DM2zYsCE5w8KFC5Pmf/CDHyRnWLJkSfIa0BFc4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMisoqMDQEdqampKmq+pqUnO8L73vS9pvry8PDnD+vXrk9d47bXXkubLyvS/APBe0NjYmLxGfX190vzrr7+enKFPnz5J8717907OUFtbmzS/Zs2a5AzV1dVJ8926dUvOUBRF8hrQEfyGAwAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkVtHRAaAjlUqlpPlddtklOUO/fv2S5ouiSM6wdu3a5DXmz5+fNF9eXp6cAQDoeDn2Jo2NjUnzr7/+enKG1L3Nq6++mpyhe/fuSfNdunRJztC/f/+k+YkTJyZneOCBB5LmFy1alJwB2sMVLgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwqOjoAdKSysrTOcY899kjOUF1dnTRfKpWSMyxbtix5jbVr1ybNr1+/PjkDAPDe0NTUlDS/YcOG5Ay/+tWvkua7deuWnKG+vj5pft99903OUFVVlTS///77J2c44YQTkuZvvvnm5Azr1q1LXoMdjytcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMKjo6AHSksrK0zrFnz57JGbp06ZI039TUlJxh6dKlyWusWLEiaT7H4wAA3htKpVLS/IYNG5IzpO6P5syZk5zhxRdfTJqfNGlScoYJEyYkzdfU1CRnOOWUU5Lmf/GLXyRnePbZZ5Pmi6JIzsC7jytcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmVV0dADoSGVlaZ1jTU1Nh2dobGxMztDQ0JC8RpcuXZLmU5+HiDzPBQCQplQqJa+Rui9oampKzrBq1aqk+RdffDE5Q+rz0L9//+QMhxxySNJ8VVVVcoYBAwYkze+zzz7JGZ577rmk+aIokjPw7uMKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgs4qODgDtVSqVktfo06dP0vx+++2XnKGiIu2fYWNjY3KG9evXJ6/Rq1evpPny8vLkDBs3bkxeAwDaK8fe5J0g9ZzcrVu35Axdu3ZNms+xP2poaOjwDGVlaX8f79u3b3KGqqqqpPkce7zUNaqrq5MzFEWRvAY7Hle4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMqvo6ADQXl27dk1eo66uLml+6NChyRlKpVLS/MaNG5MzrFixInmN2trapPlOnTolZ2hsbEyab2hoSM5QFEXyGgC8O5WVpf8tM/V8WFVVlZxh4MCBSfN77713coaKirRfU1L3BBERa9asSZrPsa/40Ic+lDQ/ceLE5Aw777xz0nyOverixYuT5ufMmZOcwR6P9nCFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQWUVHB2DHVSqVkub79OmTnGHcuHFJ8507d07O0NTUlDS/Zs2a5Azr169PXmPJkiVJ8w0NDckZUn+mAKCj9ezZM2n+wAMPTM4wZsyYpPlDDz00OUNtbW3SfJcuXZIzpOrevXuHr9G1a9fkDKn7q1dffTU5w0033ZQ0//vf/z45A7SHK1wAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZVXR0AGivlStXJq/x8ssvJ82PGTMmOUNVVVXS/IIFC5IzPP3008lrPPHEE0nzGzduTM6QqiiKjo4AwLtYU1NT8hqrV69Oml+zZk1yhp49eybNDxgwIDlD3759k+a7du2anIE3LFmyJGn+qquuSs7wox/9KGn+nbDPZMfkChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAILOKjg4A7VVfX5+8xoMPPpg0P2zYsOQMu+22W9L8Aw88kJzhvvvuS17jL3/5S9J8Y2NjcoaiKJLXAID2ynEeWrduXdL83LlzkzN06dIlaT7H83DUUUclzffp0yc5Q3l5edJ8U1NTcoZly5Ylzef4ebjqqquS5p9++unkDA0NDclrQEdwhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACAzhQsAAABAZgoXAAAAgMwULgAAAACZKVwAAAAAMlO4AAAAAGSmcAEAAADITOECAAAAkJnCBQAAACCzUlEUxXbdsVR6u7PA3115eXnSfJcuXZIzpK6xYcOG5Azr1q1LXqOxsTFpfjv/rwjeVfxc825hn/fekeN72alTp6T56urq5Ay9evVKmq+qqkrOsHbt2qT5lStXJmdI3aPlyLBx48bkNeC9Znv3eK5wAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyKxVFUWzXHUultzsLALynbOcpFjqcfR4AbL/t3eO5wgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgM4ULAAAAQGYKFwAAAIDMFC4AAAAAmSlcAAAAADJTuAAAAABkpnABAAAAyEzhAgAAAJCZwgUAAAAgs1JRFEVHhwAAAAB4L3GFCwAAAEBmChcAAACAzBQuAAAAAJkpXAAAAAAyU7gAAAAAZKZwAQAAAMhM4QIAAACQmcIFAAAAIDOFCwAAAEBm/wcuHXiIMJ1ThwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################  Only for  MNIST data :\n",
    "\n",
    "\n",
    "\n",
    "if not use_rhc_data : \n",
    "    # Visualize reconstructed images and added pixels\n",
    "    model.eval()\n",
    "    original_label = 1\n",
    "    target_label = 3\n",
    "    original_label_tensor = torch.tensor([original_label]).to(device)\n",
    "    target_label_tensor = torch.tensor([target_label]).to(device)\n",
    "    latent_dim = model.fc2_mean.out_features  # Assuming latent_dim is defined by fc2_mean's output size\n",
    "    latent_sample = torch.randn(1, latent_dim).to(device)\n",
    "    original_label_onehot = F.one_hot(original_label_tensor, num_classes=10).float()\n",
    "    target_label_onehot = F.one_hot(target_label_tensor, num_classes=10).float()\n",
    "    combined_input_original = torch.cat((latent_sample, original_label_onehot), dim=1)\n",
    "    with torch.no_grad():\n",
    "        reconstructed_image_original = model.decode(combined_input_original).view(28, 28).cpu().numpy()\n",
    "    combined_input_target = torch.cat((latent_sample, target_label_onehot), dim=1)\n",
    "    with torch.no_grad():\n",
    "        reconstructed_image_target = model.decode(combined_input_target).view(28, 28).cpu().numpy()\n",
    "    pixel_difference = reconstructed_image_target - reconstructed_image_original\n",
    "\n",
    "    # Display images\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    ax1.imshow(reconstructed_image_original, cmap='gray')\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(f\"Original Number {original_label}\")\n",
    "    ax2.imshow(reconstructed_image_target, cmap='gray')\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(f\"Reconstructed Number with Label {target_label}\")\n",
    "    #ax3.imshow(reconstructed_image_target, cmap='gray')\n",
    "    #ax3.imshow(pixel_difference, cmap='Reds', alpha=0.5, vmin=0, vmax=np.max(pixel_difference))\n",
    "    #ax3.axis('off')\n",
    "    #ax3.set_title(f\"Added Pixels (in red)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    print(\"test : end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
